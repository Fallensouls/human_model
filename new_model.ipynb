{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36964bit40d73d4f2e70422fa9f3465cc382e75e",
   "display_name": "Python 3.6.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def createDataset(input_size,status_size,output_size,slidingWindow):\n",
    "    u_seq_total = torch.zeros([0,slidingWindow,input_size])\n",
    "    y_seq_total = torch.zeros([0,output_size])\n",
    "    ABCD_seq_total = torch.zeros([0,status_size*status_size+status_size*input_size+output_size*status_size+output_size*input_size])\n",
    "    length = 5000\n",
    "    differentMatrix = 1    #生成多少组不同的ABCD\n",
    "    ABCD_range = length-slidingWindow-1    #每组ABCD用多少个序列\n",
    "    for j in range(differentMatrix):\n",
    "        u_seq = torch.zeros([0,slidingWindow,input_size])\n",
    "        y_seq = torch.zeros([0,output_size])\n",
    "        ABCD_seq = torch.zeros([0,status_size*status_size+status_size*input_size+output_size*status_size+output_size*input_size])\n",
    "        A = torch.randn(status_size,status_size)/status_size\n",
    "        B = torch.randn(status_size,input_size)/status_size\n",
    "        C = torch.randn(output_size,status_size)\n",
    "        D = torch.randn(output_size,input_size)\n",
    "\n",
    "        u = torch.randn(length,input_size)\n",
    "        x = torch.zeros(status_size,1)\n",
    "\n",
    "        for i in range(u.size()[0]):\n",
    "            y = torch.mm(C,x) + torch.mm(D,u[i].view(input_size,1))\n",
    "            x = torch.mm(A,x) + torch.mm(B,u[i].view(input_size,1))\n",
    "            u\n",
    "            if(i>slidingWindow):\n",
    "                u_seq = torch.cat([u_seq,u[i-slidingWindow:i].view(1,slidingWindow,input_size)])\n",
    "                y_seq = torch.cat([y_seq,y.view(1,output_size)])\n",
    "                ABCD = torch.cat([A.flatten(),B.flatten(),C.flatten(),D.flatten()])\n",
    "                ABCD_seq = torch.cat([ABCD_seq,ABCD.view(1,ABCD.shape[0])])\n",
    "        u_seq_total = torch.cat([u_seq_total,u_seq])\n",
    "        y_seq_total = torch.cat([y_seq_total,y_seq])\n",
    "        ABCD_seq_total = torch.cat([ABCD_seq_total,ABCD_seq])\n",
    "    return u_seq_total,y_seq_total,ABCD_seq_total,ABCD_range\n",
    "\n",
    "def createTestDataset(input_size,status_size,output_size,slidingWindow):\n",
    "    u_seq = torch.zeros([0,slidingWindow,input_size])\n",
    "    y_seq = torch.zeros([0,output_size])\n",
    "    ABCD_seq = torch.zeros([0,status_size*status_size+status_size*input_size+output_size*status_size+output_size*input_size])\n",
    "    length = 500\n",
    "    differentMatrix = 60     #生成多少组不同的ABCD\n",
    "    ABCD_range = length-slidingWindow-1    #每组ABCD用多少个序列\n",
    "    for j in range(differentMatrix):\n",
    "        A = torch.randn(status_size,status_size)/status_size\n",
    "        B = torch.randn(status_size,input_size)/status_size\n",
    "        C = torch.randn(output_size,status_size)\n",
    "        D = torch.randn(output_size,input_size)\n",
    "        ABCD = torch.cat([A.flatten(),B.flatten(),C.flatten(),D.flatten()])\n",
    "        ABCD_seq = torch.cat([ABCD_seq,ABCD.view(1,ABCD.shape[0])])\n",
    "        u = torch.randn(length,input_size)\n",
    "        x = torch.zeros(status_size,1)\n",
    "\n",
    "        for i in range(u.size()[0]):\n",
    "            y = torch.mm(C,x) + torch.mm(D,u[i].view(input_size,1))\n",
    "            x = torch.mm(A,x) + torch.mm(B,u[i].view(input_size,1))\n",
    "            if(i>slidingWindow):\n",
    "                u_seq = torch.cat([u_seq,u[i-slidingWindow:i].view(1,slidingWindow,input_size)])\n",
    "                y_seq = torch.cat([y_seq,y.view(1,output_size)])\n",
    "    return u_seq,y_seq,ABCD_seq,ABCD_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch[1/2000], loss:0.353659\n",
      "Epoch[2/2000], loss:0.353593\n",
      "Epoch[3/2000], loss:0.353527\n",
      "Epoch[4/2000], loss:0.353461\n",
      "Epoch[5/2000], loss:0.353395\n",
      "Epoch[6/2000], loss:0.353330\n",
      "Epoch[7/2000], loss:0.353264\n",
      "Epoch[8/2000], loss:0.353198\n",
      "Epoch[9/2000], loss:0.353133\n",
      "Epoch[10/2000], loss:0.353068\n",
      "Epoch[11/2000], loss:0.353002\n",
      "Epoch[12/2000], loss:0.352937\n",
      "Epoch[13/2000], loss:0.352872\n",
      "Epoch[14/2000], loss:0.352807\n",
      "Epoch[15/2000], loss:0.352743\n",
      "Epoch[16/2000], loss:0.352678\n",
      "Epoch[17/2000], loss:0.352613\n",
      "Epoch[18/2000], loss:0.352549\n",
      "Epoch[19/2000], loss:0.352484\n",
      "Epoch[20/2000], loss:0.352420\n",
      "Epoch[21/2000], loss:0.352356\n",
      "Epoch[22/2000], loss:0.352292\n",
      "Epoch[23/2000], loss:0.352228\n",
      "Epoch[24/2000], loss:0.352164\n",
      "Epoch[25/2000], loss:0.352100\n",
      "Epoch[26/2000], loss:0.352037\n",
      "Epoch[27/2000], loss:0.351973\n",
      "Epoch[28/2000], loss:0.351910\n",
      "Epoch[29/2000], loss:0.351846\n",
      "Epoch[30/2000], loss:0.351783\n",
      "Epoch[31/2000], loss:0.351720\n",
      "Epoch[32/2000], loss:0.351657\n",
      "Epoch[33/2000], loss:0.351594\n",
      "Epoch[34/2000], loss:0.351531\n",
      "Epoch[35/2000], loss:0.351468\n",
      "Epoch[36/2000], loss:0.351405\n",
      "Epoch[37/2000], loss:0.351343\n",
      "Epoch[38/2000], loss:0.351280\n",
      "Epoch[39/2000], loss:0.351218\n",
      "Epoch[40/2000], loss:0.351156\n",
      "Epoch[41/2000], loss:0.351094\n",
      "Epoch[42/2000], loss:0.351032\n",
      "Epoch[43/2000], loss:0.350970\n",
      "Epoch[44/2000], loss:0.350908\n",
      "Epoch[45/2000], loss:0.350846\n",
      "Epoch[46/2000], loss:0.350784\n",
      "Epoch[47/2000], loss:0.350723\n",
      "Epoch[48/2000], loss:0.350661\n",
      "Epoch[49/2000], loss:0.350600\n",
      "Epoch[50/2000], loss:0.350539\n",
      "Epoch[51/2000], loss:0.350478\n",
      "Epoch[52/2000], loss:0.350417\n",
      "Epoch[53/2000], loss:0.350356\n",
      "Epoch[54/2000], loss:0.350295\n",
      "Epoch[55/2000], loss:0.350234\n",
      "Epoch[56/2000], loss:0.350173\n",
      "Epoch[57/2000], loss:0.350113\n",
      "Epoch[58/2000], loss:0.350052\n",
      "Epoch[59/2000], loss:0.349992\n",
      "Epoch[60/2000], loss:0.349932\n",
      "Epoch[61/2000], loss:0.349871\n",
      "Epoch[62/2000], loss:0.349811\n",
      "Epoch[63/2000], loss:0.349751\n",
      "Epoch[64/2000], loss:0.349692\n",
      "Epoch[65/2000], loss:0.349632\n",
      "Epoch[66/2000], loss:0.349572\n",
      "Epoch[67/2000], loss:0.349513\n",
      "Epoch[68/2000], loss:0.349453\n",
      "Epoch[69/2000], loss:0.349394\n",
      "Epoch[70/2000], loss:0.349334\n",
      "Epoch[71/2000], loss:0.349275\n",
      "Epoch[72/2000], loss:0.349216\n",
      "Epoch[73/2000], loss:0.349157\n",
      "Epoch[74/2000], loss:0.349098\n",
      "Epoch[75/2000], loss:0.349039\n",
      "Epoch[76/2000], loss:0.348981\n",
      "Epoch[77/2000], loss:0.348922\n",
      "Epoch[78/2000], loss:0.348864\n",
      "Epoch[79/2000], loss:0.348805\n",
      "Epoch[80/2000], loss:0.348747\n",
      "Epoch[81/2000], loss:0.348689\n",
      "Epoch[82/2000], loss:0.348631\n",
      "Epoch[83/2000], loss:0.348573\n",
      "Epoch[84/2000], loss:0.348515\n",
      "Epoch[85/2000], loss:0.348457\n",
      "Epoch[86/2000], loss:0.348399\n",
      "Epoch[87/2000], loss:0.348341\n",
      "Epoch[88/2000], loss:0.348284\n",
      "Epoch[89/2000], loss:0.348226\n",
      "Epoch[90/2000], loss:0.348169\n",
      "Epoch[91/2000], loss:0.348112\n",
      "Epoch[92/2000], loss:0.348055\n",
      "Epoch[93/2000], loss:0.347998\n",
      "Epoch[94/2000], loss:0.347941\n",
      "Epoch[95/2000], loss:0.347884\n",
      "Epoch[96/2000], loss:0.347827\n",
      "Epoch[97/2000], loss:0.347770\n",
      "Epoch[98/2000], loss:0.347714\n",
      "Epoch[99/2000], loss:0.347657\n",
      "Epoch[100/2000], loss:0.347601\n",
      "Epoch[101/2000], loss:0.347545\n",
      "Epoch[102/2000], loss:0.347488\n",
      "Epoch[103/2000], loss:0.347432\n",
      "Epoch[104/2000], loss:0.347376\n",
      "Epoch[105/2000], loss:0.347320\n",
      "Epoch[106/2000], loss:0.347264\n",
      "Epoch[107/2000], loss:0.347209\n",
      "Epoch[108/2000], loss:0.347153\n",
      "Epoch[109/2000], loss:0.347097\n",
      "Epoch[110/2000], loss:0.347042\n",
      "Epoch[111/2000], loss:0.346987\n",
      "Epoch[112/2000], loss:0.346931\n",
      "Epoch[113/2000], loss:0.346876\n",
      "Epoch[114/2000], loss:0.346821\n",
      "Epoch[115/2000], loss:0.346766\n",
      "Epoch[116/2000], loss:0.346711\n",
      "Epoch[117/2000], loss:0.346656\n",
      "Epoch[118/2000], loss:0.346601\n",
      "Epoch[119/2000], loss:0.346547\n",
      "Epoch[120/2000], loss:0.346492\n",
      "Epoch[121/2000], loss:0.346438\n",
      "Epoch[122/2000], loss:0.346383\n",
      "Epoch[123/2000], loss:0.346329\n",
      "Epoch[124/2000], loss:0.346275\n",
      "Epoch[125/2000], loss:0.346221\n",
      "Epoch[126/2000], loss:0.346167\n",
      "Epoch[127/2000], loss:0.346113\n",
      "Epoch[128/2000], loss:0.346059\n",
      "Epoch[129/2000], loss:0.346005\n",
      "Epoch[130/2000], loss:0.345952\n",
      "Epoch[131/2000], loss:0.345898\n",
      "Epoch[132/2000], loss:0.345845\n",
      "Epoch[133/2000], loss:0.345791\n",
      "Epoch[134/2000], loss:0.345738\n",
      "Epoch[135/2000], loss:0.345685\n",
      "Epoch[136/2000], loss:0.345632\n",
      "Epoch[137/2000], loss:0.345579\n",
      "Epoch[138/2000], loss:0.345526\n",
      "Epoch[139/2000], loss:0.345473\n",
      "Epoch[140/2000], loss:0.345420\n",
      "Epoch[141/2000], loss:0.345368\n",
      "Epoch[142/2000], loss:0.345315\n",
      "Epoch[143/2000], loss:0.345262\n",
      "Epoch[144/2000], loss:0.345210\n",
      "Epoch[145/2000], loss:0.345158\n",
      "Epoch[146/2000], loss:0.345105\n",
      "Epoch[147/2000], loss:0.345053\n",
      "Epoch[148/2000], loss:0.345001\n",
      "Epoch[149/2000], loss:0.344949\n",
      "Epoch[150/2000], loss:0.344897\n",
      "Epoch[151/2000], loss:0.344846\n",
      "Epoch[152/2000], loss:0.344794\n",
      "Epoch[153/2000], loss:0.344742\n",
      "Epoch[154/2000], loss:0.344691\n",
      "Epoch[155/2000], loss:0.344639\n",
      "Epoch[156/2000], loss:0.344588\n",
      "Epoch[157/2000], loss:0.344537\n",
      "Epoch[158/2000], loss:0.344485\n",
      "Epoch[159/2000], loss:0.344434\n",
      "Epoch[160/2000], loss:0.344383\n",
      "Epoch[161/2000], loss:0.344332\n",
      "Epoch[162/2000], loss:0.344281\n",
      "Epoch[163/2000], loss:0.344231\n",
      "Epoch[164/2000], loss:0.344180\n",
      "Epoch[165/2000], loss:0.344129\n",
      "Epoch[166/2000], loss:0.344079\n",
      "Epoch[167/2000], loss:0.344028\n",
      "Epoch[168/2000], loss:0.343978\n",
      "Epoch[169/2000], loss:0.343928\n",
      "Epoch[170/2000], loss:0.343878\n",
      "Epoch[171/2000], loss:0.343828\n",
      "Epoch[172/2000], loss:0.343777\n",
      "Epoch[173/2000], loss:0.343728\n",
      "Epoch[174/2000], loss:0.343678\n",
      "Epoch[175/2000], loss:0.343628\n",
      "Epoch[176/2000], loss:0.343578\n",
      "Epoch[177/2000], loss:0.343529\n",
      "Epoch[178/2000], loss:0.343479\n",
      "Epoch[179/2000], loss:0.343430\n",
      "Epoch[180/2000], loss:0.343380\n",
      "Epoch[181/2000], loss:0.343331\n",
      "Epoch[182/2000], loss:0.343282\n",
      "Epoch[183/2000], loss:0.343233\n",
      "Epoch[184/2000], loss:0.343184\n",
      "Epoch[185/2000], loss:0.343135\n",
      "Epoch[186/2000], loss:0.343086\n",
      "Epoch[187/2000], loss:0.343037\n",
      "Epoch[188/2000], loss:0.342988\n",
      "Epoch[189/2000], loss:0.342940\n",
      "Epoch[190/2000], loss:0.342891\n",
      "Epoch[191/2000], loss:0.342843\n",
      "Epoch[192/2000], loss:0.342794\n",
      "Epoch[193/2000], loss:0.342746\n",
      "Epoch[194/2000], loss:0.342698\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ba6b2f9c330d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# 向前传播\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mABCD_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-ba6b2f9c330d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, u, y)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mx_invy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_invy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mx_invy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpinverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mslw\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_invy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "class LSTMLinear(nn.Module):\n",
    "    def __init__(self,input_size,status_size,output_size,slidingWindow):\n",
    "        super(LSTMLinear,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.status_size = status_size\n",
    "        self.output_si\n",
    "        ze = output_size\n",
    "        self.slidingWindow = slidingWindow\n",
    "        self.lstm = nn.LSTM(self.input_size,self.input_size,1,batch_first=True,bidirectional=True,dropout=0.2)\n",
    "        self.fc1 = nn.Linear(self.input_size*self.slidingWindow*2,self.input_size*self.slidingWindow)\n",
    "        ABCD = status_size*status_size+status_size*input_size+output_size*status_size+output_size*input_size     #ABCD总变量数\n",
    "        self.fc2 = nn.Linear(self.input_size*self.slidingWindow*self.output_size,ABCD)\n",
    "        \n",
    "    def forward(self,u,y):\n",
    "        x,_ = self.lstm(u)\n",
    "        batch,slw,input_feature = x.shape\n",
    "        x = self.fc1(x.reshape(batch,slw*input_feature))\n",
    "        x_invy = torch.zeros(batch,slw*self.input_size*self.output_size)\n",
    "        x_invy = x_invy.cuda()\n",
    "        for i in range(batch):\n",
    "            x_invy[i] = torch.mm(torch.pinverse(x[i].view(1,slw*self.input_size)),y[i].view(1,self.output_size)).flatten()\n",
    "        res = self.fc2(x_invy)\n",
    "        return res\n",
    "input_size = 10\n",
    "status_size = 20\n",
    "output_size = 10\n",
    "slidingWindow = 50\n",
    "assert(input_size == output_size)\n",
    "\n",
    "(u_seq,y_seq,ABCD_seq,ABCD_range) = createDataset(input_size,status_size,output_size,slidingWindow)\n",
    "model = LSTMLinear(input_size,status_size,output_size,slidingWindow).cuda()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=5e-3)\n",
    "\n",
    "num_epochs = 2000\n",
    "batch_size = 64\n",
    "for epoch in range(num_epochs):\n",
    "    if torch.cuda.is_available():\n",
    "        u_seq = u_seq.cuda()\n",
    "        y_seq = y_seq.cuda()\n",
    "        ABCD_seq = ABCD_seq.cuda()\n",
    "    else:\n",
    "        u_seq = u_seq\n",
    "        y_seq = y_seq\n",
    "        ABCD_seq = ABCD_seq\n",
    "    for i in range(int(len(u_seq)/batch_size)):\n",
    "        u = u_seq[i*batch_size:(i+1)*batch_size]\n",
    "        y = y_seq[i*batch_size:(i+1)*batch_size]\n",
    "        # 向前传播\n",
    "        out = model(u,y)\n",
    "        loss = criterion(out,ABCD_seq[i*batch_size:(i+1)*batch_size])\n",
    "    \n",
    "        # 向后传播\n",
    "        optimizer.zero_grad() # 注意每次迭代都需要清零\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if (epoch+1) %1 == 0:\n",
    "        print('Epoch[{}/{}], loss:{:.6f}'.format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "source": [
    "(u_test_seq,y_test_seq,ABCD_test_seq,ABCD_range) = createTestDataset(input_size,status_size,output_size,slidingWindow)\n",
    "u = u_test_seq[0:batch_size]\n",
    "y = y_test_seq[0:batch_size]\n",
    "u = u.cuda()\n",
    "y = y.cuda()\n",
    "output = model(u,y)\n",
    "\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 41,
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-583adb6f7acf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mu_test_seq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test_seq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mABCD_test_seq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mABCD_range\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreateTestDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstatus_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mslidingWindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu_test_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-ffa819d7486b>\u001b[0m in \u001b[0;36mcreateTestDataset\u001b[0;34m(input_size, status_size, output_size, slidingWindow)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mslidingWindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                 \u001b[0mu_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mu_seq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mslidingWindow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mslidingWindow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m                 \u001b[0my_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_seq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mu_seq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_seq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mABCD_seq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mABCD_range\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "gt_show = [k.cpu().detach().numpy() for k in ABCD_test_seq[0]]\n",
    "pred_show = [k.cpu().detach().numpy() for k in output[0]]\n",
    "print(len(gt_show),len(pred_show))\n",
    "# train_gt_show = [i.cpu().detach().numpy() for k in train_gt for i in k]\n",
    "# train_pred_show = [i.cpu().detach().numpy() for k in train_pred for i in k]\n",
    "length = 400\n",
    "# plt.scatter(list(range(length)), train_gt_show[0:length], c='b')\n",
    "# plt.scatter(list(range(length)), train_pred_show[0:length], c='r')\n",
    "plt.scatter(list(range(length)), gt_show[500:length+500], c='b')\n",
    "plt.scatter(list(range(length)), pred_show[500:length+500\n",
    "], c='r')\n",
    "plt.xlabel('point')\n",
    "plt.ylabel('output')\n",
    "# plt.title(\" system_dim {} output_dim {} delay {}\".format( system_dim, output_dim, delay))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "from data import get_data, gen_label\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models\n",
    "import cv2\n",
    "\n",
    "class CLSTMModel(torch.nn.Module):\n",
    "    def __init__(self,feature_num,slidingWindow,InformationAdd=0,InformationLack=0):\n",
    "        super(CLSTMModel, self).__init__()\n",
    "        self.feature_num = feature_num\n",
    "        self.cnn_layer = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, self.feature_num)\n",
    "        )\n",
    "\n",
    "#         model = models.resnet18(pretrained=True)\n",
    "# #         model.fc = nn.Identity()\n",
    "#         self.cnn_layer = model\n",
    "#         for p in self.parameters():\n",
    "#             p.requires_grad=False\n",
    "#         self.cnn_layer.fc = nn.Linear(512,self.feature_num)\n",
    "        self.slidingWindow = slidingWindow\n",
    "        self.InformationAdd = InformationAdd\n",
    "        self.InformationLack = InformationLack\n",
    "        self.lstm = nn.LSTM(self.feature_num,self.feature_num,1,batch_first=True,bidirectional=True,dropout=0.2)\n",
    "        self.fc = nn.Linear(self.feature_num*2,1)\n",
    "        self.slidingFc = nn.Linear(slidingWindow-self.InformationLack+self.InformationAdd,1)\n",
    "        \n",
    "        \n",
    "    def forward(self, u):\n",
    "        # cnn_x = self.cnn_layer(x)\n",
    "        # cnn_x = cnn_x.unsqueeze(0)\n",
    "        x, (h_out, c_out) = self.lstm(u)\n",
    "        # h = h_out.permute(1, 0, 2)\n",
    "        # h = h.reshape(h.shape[0], -1)\n",
    "        # h = self.fc(h)\n",
    "        \n",
    "        b,s,h = x.shape\n",
    "        x = x.reshape(s*b, h) # 转换成线性层的输入格式\n",
    "        x = self.fc(x)\n",
    "        x = x.reshape(b,s)\n",
    "        # x = self.slidingFc(x)\n",
    "        # x = x.view(b, -1)\n",
    "        return x\n",
    "        # return h, h_out, c_out   \n",
    "\n",
    "slidingWindow = 10\n",
    "seq_len = 10\n",
    "InformationAdd = 0\n",
    "InformationLack = 0\n",
    "input_size = 10\n",
    "status_size = 20\n",
    "output_size = 10\n",
    "model = CLSTMModel(feature_num, slidingWindow, InformationAdd, InformationLack).cuda()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 10\n",
    "delay = 10\n",
    "\n",
    "pbar = tqdm(range(1, epochs+1))\n",
    "\n",
    "\n",
    "gen_label(system_dim,output_dim,batch_size,False)  # 改变参数需要重新生成label，设reset=True\n",
    "train_loader, test_loader = get_data(seq_len, batch_size, delay)\n",
    "for epoch in pbar:\n",
    "    # h0 = torch.randn(2, batch_size-slidingWindow, feature_num).cuda()\n",
    "    # c0 = torch.randn(2, batch_size-slidingWindow, feature_num).cuda()\n",
    "    for batch_index, (images, labels) in enumerate(train_loader):\n",
    "        images = images.cuda()\n",
    "        labels = labels.cuda()\n",
    "        outputs = model(images)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs.squeeze(0), labels)\n",
    "        loss.backward()\n",
    "        pbar.set_postfix({'train loss' : '{0:1.5f}'.format(loss)})\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def createDataset(input_size,status_size,output_size):\n",
    "    length = 5000\n",
    "    y_seq = torch.zeros([length,output_size])\n",
    "\n",
    "    A = torch.randn(status_size,status_size)/status_size\n",
    "    B = torch.randn(status_size,input_size)/status_size\n",
    "    C = torch.randn(output_size,status_size)\n",
    "    D = torch.randn(output_size,input_size)\n",
    "    u = torch.randn(length,input_size)\n",
    "    x = torch.zeros(status_size,1)\n",
    "    for i in range(length):\n",
    "        y = torch.mm(C,x) + torch.mm(D,u[i].view(input_size,1))\n",
    "        x = torch.mm(A,x) + torch.mm(B,u[i].view(input_size,1))\n",
    "        # u_seq = torch.cat([u_seq,u[i-slidingWindow:i].view(1,slidingWindow,input_size)])\n",
    "        y_seq[i] = y[:,0]\n",
    "    return u,y_seq\n",
    "\n",
    "def createTestDataset(input_size,status_size,output_size,slidingWindow):\n",
    "    u_seq = torch.zeros([0,slidingWindow,input_size])\n",
    "    y_seq = torch.zeros([0,output_size])\n",
    "    ABCD_seq = torch.zeros([0,status_size*status_size+status_size*input_size+output_size*status_size+output_size*input_size])\n",
    "    length = 500\n",
    "    differentMatrix = 60     #生成多少组不同的ABCD\n",
    "    ABCD_range = length-slidingWindow-1    #每组ABCD用多少个序列\n",
    "    for j in range(differentMatrix):\n",
    "        A = torch.randn(status_size,status_size)/status_size\n",
    "        B = torch.randn(status_size,input_size)/status_size\n",
    "        C = torch.randn(output_size,status_size)\n",
    "        D = torch.randn(output_size,input_size)\n",
    "        ABCD = torch.cat([A.flatten(),B.flatten(),C.flatten(),D.flatten()])\n",
    "        ABCD_seq = torch.cat([ABCD_seq,ABCD.view(1,ABCD.shape[0])])\n",
    "        u = torch.randn(length,input_size)\n",
    "        x = torch.zeros(status_size,1)\n",
    "\n",
    "        for i in range(u.size()[0]):\n",
    "            y = torch.mm(C,x) + torch.mm(D,u[i].view(input_size,1))\n",
    "            x = torch.mm(A,x) + torch.mm(B,u[i].view(input_size,1))\n",
    "            if(i>slidingWindow):\n",
    "                u_seq = torch.cat([u_seq,u[i-slidingWindow:i].view(1,slidingWindow,input_size)])\n",
    "                y_seq = torch.cat([y_seq,y.view(1,output_size)])\n",
    "    return u_seq,y_seq,ABCD_seq,ABCD_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 20/20 [09:35<00:00, 28.78s/it, train loss=0.00874]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "from data import get_data, gen_label\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models\n",
    "import cv2\n",
    "\n",
    "class CLSTMModel(torch.nn.Module):\n",
    "    def __init__(self,input_size,status_size,output_size,InformationAdd=0,InformationLack=0):\n",
    "        super(CLSTMModel, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.status_size = status_size\n",
    "        # self.cnn_layer = nn.Sequential(\n",
    "        #     nn.Conv2d(3, 32, kernel_size=3),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(32, 64, kernel_size=3, stride=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(64, 64, kernel_size=3, stride=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(64, 128, kernel_size=3, stride=2),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Flatten(),\n",
    "        #     nn.Linear(512, self.feature_num)\n",
    "        # )\n",
    "\n",
    "#         model = models.resnet18(pretrained=True)\n",
    "# #         model.fc = nn.Identity()\n",
    "#         self.cnn_layer = model\n",
    "#         for p in self.parameters():\n",
    "#             p.requires_grad=False\n",
    "#         self.cnn_layer.fc = nn.Linear(512,self.feature_num)\n",
    "        \n",
    "        self.output_size = output_size\n",
    "        self.InformationAdd = InformationAdd\n",
    "        self.InformationLack = InformationLack\n",
    "        self.lstm = nn.LSTM(self.input_size,self.input_size,1,batch_first=True,bidirectional=True,dropout=0.2)\n",
    "        self.fc = nn.Linear(self.input_size*2,self.status_size)\n",
    "        self.fc2 = nn.Linear(self.status_size+input_size,self.output_size)\n",
    "        self.slidingFc = nn.Linear(slidingWindow-self.InformationLack+self.InformationAdd,1)\n",
    "        \n",
    "        \n",
    "    def forward(self, u):\n",
    "        # cnn_x = self.cnn_layer(x)\n",
    "        # cnn_x = cnn_x.unsqueeze(0)\n",
    "        x, (h_out, c_out) = self.lstm(u.unsqueeze(0))\n",
    "        # h = h_out.permute(1, 0, 2)\n",
    "        # h = h.reshape(h.shape[0], -1)\n",
    "        # h = self.fc(h)\n",
    "        b,s,h = x.shape\n",
    "        x = x.reshape(s,h) # 转换成线性层的输入格式\n",
    "\n",
    "        x = self.fc(x)\n",
    "        x = torch.cat([x,u],1)\n",
    "        x = self.fc2(x)\n",
    "        x = x.reshape(s,self.output_size)\n",
    "        # x = self.slidingFc(x)\n",
    "        # x = x.view(b, -1)\n",
    "        return x\n",
    "        # return h, h_out, c_out   \n",
    "\n",
    "slidingWindow = 50\n",
    "seq_len = 10\n",
    "InformationAdd = 0\n",
    "InformationLack = 0\n",
    "input_size = 10\n",
    "status_size = 20\n",
    "output_size = 10\n",
    "model = CLSTMModel(input_size,status_size, output_size, InformationAdd, InformationLack).cuda()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 20\n",
    "delay = 10\n",
    "\n",
    "pbar = tqdm(range(1, epochs+1))\n",
    "\n",
    "u,y_seq = createDataset(input_size,status_size,output_size)\n",
    "u = u.cuda()\n",
    "y_seq = y_seq.cuda()\n",
    "for epoch in pbar:\n",
    "    # h0 = torch.randn(2, batch_size-slidingWindow, feature_num).cuda()\n",
    "    # c0 = torch.randn(2, batch_size-slidingWindow, feature_num).cuda()\n",
    "    for i in range(u.size()[0]-slidingWindow):\n",
    "        input = u[i:i+slidingWindow]\n",
    "        y = y_seq[i:i+slidingWindow]\n",
    "        input = input.cuda()\n",
    "        y = y.cuda()\n",
    "        outputs = model(input)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(y,outputs)\n",
    "        loss.backward()\n",
    "        pbar.set_postfix({'train loss' : '{0:1.5f}'.format(loss)})\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = model.state_dict()[\"fc2.weight\"]\n",
    "C, D = w.split([status_size, input_size], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-1.4827e+00,  3.0155e-02, -8.4355e-01, -2.3534e-01, -1.1329e-01,\n          1.3857e-01, -1.7594e+00,  1.9898e-01, -5.9756e-01, -3.0593e-01,\n         -1.5941e+00, -4.8806e-01,  8.4542e-01, -6.9222e-01,  7.0619e-01,\n          8.0864e-01, -4.3364e-01,  1.3554e+00,  8.0440e-01,  9.5256e-01],\n        [-3.8697e-01,  3.4242e-01, -1.1378e+00,  9.3182e-01,  5.1890e-01,\n          1.2960e-03,  2.0555e-01,  8.4407e-02, -1.5373e+00,  9.7531e-01,\n          1.0173e-01, -9.2757e-01, -9.5204e-01,  2.8672e-01,  3.4223e-01,\n         -1.6956e-01,  2.2059e-01, -8.8120e-01, -1.9009e+00, -2.2415e-01],\n        [-4.1541e-01, -4.7511e-01, -1.3011e+00, -6.6642e-02, -3.7115e-01,\n          3.8658e-01, -1.2375e+00, -1.2711e+00, -1.4697e+00,  2.7861e-01,\n         -1.0509e+00,  6.7557e-01, -4.1998e-02,  1.4125e+00, -6.0599e-01,\n          8.1215e-01, -1.1208e-01,  6.2867e-01, -4.0436e-01, -3.3728e-01],\n        [-1.4426e-01, -1.5524e+00, -1.9514e+00,  1.0499e+00,  2.8590e-01,\n          5.4432e-01, -4.6377e-01,  1.0418e+00, -1.3669e-01,  7.6354e-01,\n         -2.4269e-02,  2.0880e+00,  6.5166e-01,  3.0937e-01, -7.6633e-01,\n         -7.3608e-01, -1.6525e+00,  6.5355e-01, -3.0754e-01, -4.6024e-02],\n        [ 3.2415e-01,  9.4179e-01,  1.6026e+00,  7.1448e-01,  5.6482e-01,\n          9.7633e-01, -7.8758e-01,  1.4058e-01,  8.7454e-01, -1.3468e+00,\n         -1.3490e+00,  1.3694e+00, -1.6188e+00, -5.6888e-01, -1.1294e+00,\n          2.6926e+00,  5.3285e-01,  2.4543e-01, -1.2373e+00, -6.7993e-01],\n        [-9.4299e-01, -1.6873e+00,  1.1422e+00,  1.0001e+00, -3.3466e-01,\n          9.8760e-01,  5.2030e-01, -7.0858e-01,  5.3713e-01,  1.0812e+00,\n         -6.1986e-02, -1.4146e+00,  2.5425e-01, -1.1283e+00,  1.4989e-01,\n         -4.3034e-01,  9.8039e-01,  1.6159e+00, -1.5580e+00, -7.3350e-01],\n        [ 4.4881e-01,  1.0119e+00, -8.5712e-01, -1.2112e+00, -9.1103e-01,\n          1.5558e+00,  3.3240e-01,  1.2460e+00, -4.2071e-01, -1.6623e+00,\n          1.1359e+00,  6.0566e-01,  8.3716e-01,  7.7840e-01, -9.1383e-01,\n         -5.8550e-01,  4.9371e-01,  5.4126e-02, -3.8223e-01, -9.3885e-01],\n        [ 1.2374e+00, -6.5122e-01, -6.9237e-01, -1.6914e+00,  1.4367e+00,\n         -1.2284e+00,  8.9038e-01,  1.5483e+00,  8.0422e-01, -1.2863e+00,\n          6.9648e-01,  1.0634e+00,  9.5317e-01,  3.3472e-01, -1.4028e+00,\n          3.0196e-01,  1.7989e+00,  6.5651e-01, -1.3417e+00,  7.6606e-01],\n        [ 1.5034e+00, -1.1769e+00,  1.7444e+00,  5.6908e-01, -4.6663e-01,\n          4.5672e-02,  1.5755e+00, -1.1237e+00, -2.4014e-01,  1.0347e+00,\n         -1.4055e+00,  5.6155e-01, -7.6752e-01,  6.0002e-01, -9.6735e-01,\n         -2.8411e-01, -1.0371e-01,  3.2822e-01,  5.5830e-01, -9.8339e-01],\n        [-1.5582e+00,  2.8585e-01,  1.2248e+00,  8.7823e-01, -7.1174e-01,\n          1.0436e-01,  1.2177e+00,  8.6452e-01, -1.4886e+00, -2.1137e+00,\n         -1.3863e+00, -1.2609e+00,  7.5346e-01, -4.5873e-01, -6.7714e-01,\n         -4.7624e-01,  1.4126e-01, -1.8092e+00, -8.9315e-01, -5.1348e-01]],\n       device='cuda:0') tensor([[ 0.0862,  0.6249, -1.0700, -0.2890,  0.2567,  0.3288, -0.1129,  1.1893,\n          1.3297,  0.3854],\n        [-1.6176,  0.2825,  0.7184, -0.4080,  0.4010, -0.0974,  0.2021,  1.1969,\n          0.2253, -0.4824],\n        [ 0.7882,  0.1761,  0.6250, -0.1395,  0.4162, -0.6515,  1.3217,  0.8292,\n          0.0082,  0.6716],\n        [-0.3884,  0.6007,  0.4608, -1.4516, -0.8217, -0.7983,  0.9975, -0.5163,\n         -0.2923,  1.1891],\n        [-1.1871,  0.3610,  0.8772,  0.5654, -0.0428, -0.6226,  0.2915, -0.1029,\n          0.6665,  1.4495],\n        [ 0.3684,  1.5209,  0.0825, -0.0679,  2.0837,  0.1439,  0.1938,  0.6908,\n          0.5276, -0.2707],\n        [-1.4083,  0.4982,  0.6821, -0.3835,  0.5336,  0.9831,  0.5540, -1.0093,\n          1.5915, -0.2226],\n        [-0.4102,  1.2278,  0.4857, -0.0969,  1.4092, -1.1220, -1.0216, -1.2324,\n          0.0347,  0.1081],\n        [-0.2728,  0.6832, -0.8766, -1.3688, -0.4899,  2.1652, -0.1498,  0.4590,\n         -1.2470,  0.2185],\n        [ 0.2803, -0.1435,  0.1047, -1.0212,  1.1417, -0.4712,  1.4956, -1.8809,\n         -0.1997,  0.6568]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(C,D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}